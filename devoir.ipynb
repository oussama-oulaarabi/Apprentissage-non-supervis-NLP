{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b4958b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize # Important pour l'étape 3\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# ÉTAPE 0 : DONNÉES ET PRÉTRAITEMENT (Tokenisation)\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "# Corpus de texte d'exemple (le même que le vôtre)\n",
    "corpus = [\n",
    "    \"Ceci est un exemple de phrase.\",\n",
    "    \"Un autre exemple de phrase.\",\n",
    "    \"Le modèle CBOW est utilisé pour prédire des mots en fonction du contexte.\",\n",
    "]\n",
    "\n",
    "# Prétraitement : tokenisation et création du vocabulaire\n",
    "# (C'est la fonction 'tokenize' que vous vouliez voir incluse)\n",
    "def tokenize(corpus):\n",
    "    tokens = [sentence.lower().split() for sentence in corpus]\n",
    "    vocab = set([word for sentence in tokens for word in sentence])\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "    return tokens, word2idx, idx2word\n",
    "\n",
    "# Exécution de la tokenisation\n",
    "tokens, word2idx, idx2word = tokenize(corpus)\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "print(f\"Taille du vocabulaire: {vocab_size}\")\n",
    "print(f\"Dictionnaire (mot -> index): {word2idx}\\n\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# ÉTAPE 1 : ADAPTATION VERS LE MODÈLE SKIP-GRAM\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "# 1.1. Préparer les données (mot -> contexte)\n",
    "def create_skipgram_pairs(tokens, context_size=2):\n",
    "    \"\"\"\n",
    "    Crée des paires (mot_cible, mot_contexte) pour le modèle Skip-gram.\n",
    "    \"\"\"\n",
    "    skipgram_pairs = []\n",
    "    for sentence in tokens:\n",
    "        for i in range(len(sentence)):\n",
    "            target_word = sentence[i]\n",
    "            start = max(0, i - context_size)\n",
    "            end = min(len(sentence), i + context_size + 1)\n",
    "            \n",
    "            for j in range(start, end):\n",
    "                if i == j: # Ne pas appairer un mot avec lui-même\n",
    "                    continue\n",
    "                context_word = sentence[j]\n",
    "                skipgram_pairs.append((target_word, context_word))\n",
    "    return skipgram_pairs\n",
    "\n",
    "# 1.2. Encoder les paires\n",
    "def encode_skipgram_pairs(skipgram_pairs, word2idx):\n",
    "    \"\"\"\n",
    "    Encode les paires (mot, mot) en (idx, idx).\n",
    "    \"\"\"\n",
    "    encoded_pairs = []\n",
    "    for target_word, context_word in skipgram_pairs:\n",
    "        if target_word in word2idx and context_word in word2idx:\n",
    "            target_idx = word2idx[target_word]\n",
    "            context_idx = word2idx[context_word]\n",
    "            encoded_pairs.append((target_idx, context_idx))\n",
    "    return encoded_pairs\n",
    "\n",
    "# 1.3. Définition du modèle Pytorch Skip-gram\n",
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        # La matrice d'embedding que l'on cherche à entraîner\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # La couche de sortie\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, target_idx):\n",
    "        embeds = self.embeddings(target_idx)\n",
    "        out = self.linear(embeds)\n",
    "        log_probs = torch.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "# 1.4. Boucle d'entraînement\n",
    "def train_skipgram(model, encoded_pairs, epochs=100, learning_rate=0.01):\n",
    "    loss_function = nn.NLLLoss() # 'Negative Log Likelihood Loss'\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    print(\"--- Début de l'entraînement Skip-gram ---\")\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for target_idx, context_idx in encoded_pairs:\n",
    "            \n",
    "            # Préparer les tenseurs (Entrée = Cible, Sortie = Contexte)\n",
    "            target_var = torch.tensor([target_idx], dtype=torch.long)\n",
    "            context_var = torch.tensor([context_idx], dtype=torch.long)\n",
    "\n",
    "            # Forward pass\n",
    "            model.zero_grad()\n",
    "            log_probs = model(target_var)\n",
    "\n",
    "            # Calcul de la perte (comparer la prédiction au mot de contexte)\n",
    "            loss = loss_function(log_probs, context_var)\n",
    "\n",
    "            # Backward pass et optimisation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        if epoch % 20 == 0 or epoch == epochs - 1:\n",
    "            print(f'Époque {epoch:03d}, Perte (Loss): {total_loss:.4f}')\n",
    "    print(\"--- Fin de l'entraînement ---\")\n",
    "\n",
    "# --- Exécution de l'Étape 1 ---\n",
    "embedding_dim = 10 # 10 dimensions pour les vecteurs de mots\n",
    "context_size = 2\n",
    "\n",
    "skipgram_pairs = create_skipgram_pairs(tokens, context_size)\n",
    "encoded_pairs = encode_skipgram_pairs(skipgram_pairs, word2idx)\n",
    "\n",
    "model = SkipGramModel(vocab_size, embedding_dim)\n",
    "train_skipgram(model, encoded_pairs, epochs=100)\n",
    "\n",
    "# Extraire la matrice d'embedding finale (le résultat de l'étape 1)\n",
    "embedding_matrix = model.embeddings.weight.detach().numpy()\n",
    "print(f\"\\nMatrice d'embedding (Taille: {embedding_matrix.shape}) extraite.\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# ÉTAPE 2 : VISUALISATION (ACP / PCA)\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "print(\"\\n--- Début de l'Étape 2 : Visualisation ACP (PCA) ---\")\n",
    "\n",
    "# Appliquer l'ACP pour réduire de 'embedding_dim' à 2 dimensions\n",
    "pca = PCA(n_components=2)\n",
    "embedding_2d = pca.fit_transform(embedding_matrix)\n",
    "\n",
    "# Créer la visualisation\n",
    "plt.figure(figsize=(14, 10))\n",
    "for i in range(vocab_size):\n",
    "    x, y = embedding_2d[i]\n",
    "    plt.scatter(x, y, color='blue')\n",
    "    plt.annotate(idx2word[i], # Texte (le mot)\n",
    "                 (x, y),       # Position\n",
    "                 ha='center',  # Alignement horizontal\n",
    "                 xytext=(0, 5),# Décalage du texte\n",
    "                 textcoords='offset points')\n",
    "    \n",
    "plt.title(\"Visualisation 2D des Embeddings de Mots (via ACP/PCA)\")\n",
    "plt.xlabel(\"Composante Principale 1\")\n",
    "plt.ylabel(\"Composante Principale 2\")\n",
    "plt.grid(True)\n",
    "plt.show() # Affiche le graphique\n",
    "\n",
    "print(\"Visualisation ACP terminée.\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# ÉTAPE 3 : CLUSTERING (K-MEANS)\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "print(\"\\n--- Début de l'Étape 3 : Clustering K-Means ---\")\n",
    "\n",
    "# !!! ATTENTION (votre consigne) !!!\n",
    "# Nous devons utiliser la Similarité Cosinus.\n",
    "# L'astuce consiste à normaliser les vecteurs (norme L2).\n",
    "# KMeans (qui utilise la distance euclidienne) sur des données normalisées\n",
    "# est mathématiquement équivalent à un clustering basé sur la similarité cosinus.\n",
    "embedding_matrix_normalized = normalize(embedding_matrix, norm='l2', axis=1)\n",
    "print(\"Matrice d'embedding normalisée (pour similarité cosinus).\")\n",
    "\n",
    "# Choisir 'k' (nombre de groupes)\n",
    "# En se basant sur la visualisation ACP, on peut essayer k=3 ou k=4\n",
    "k = 3 \n",
    "kmeans = KMeans(n_clusters=k, random_state=42, n_init=10) # n_init=10 évite un avertissement\n",
    "clusters = kmeans.fit_predict(embedding_matrix_normalized)\n",
    "\n",
    "# 4. Interpréter les groupes\n",
    "grouped_words = {i: [] for i in range(k)}\n",
    "\n",
    "# 'idx2word' est {0: 'mot1', 1: 'mot2', ...}\n",
    "# 'clusters' est [0, 2, 1, 0, ...] (le groupe de chaque mot)\n",
    "for word_idx, cluster_id in enumerate(clusters):\n",
    "    word = idx2word[word_idx]\n",
    "    grouped_words[cluster_id].append(word)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(f\"\\n--- Interprétation des {k} groupes (K-Means) ---\")\n",
    "for i in range(k):\n",
    "    print(f\"\\n--- GROUPE {i+1} ---\")\n",
    "    # Affiche les mots du groupe\n",
    "    print(\", \".join(grouped_words[i]))\n",
    "\n",
    "print(\"\\n(L'interprétation sera meilleure avec un corpus plus grand)\")\n",
    "print(\"--- Fin du devoir ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
